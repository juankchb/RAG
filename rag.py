# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k-q_hH7W5apPN41QpU4VV_wY3y6ny5Nb
"""

# Instalar dependencias en Colab
!apt-get install -y tesseract-ocr
!pip install pymupdf pytesseract sentence-transformers faiss-cpu googletrans==4.0.0-rc1
!pip install fitz
!pip install tools
!pip install --upgrade pymupdf

!pip install --upgrade pymupdf

import pymupdf  # PyMuPDF
import pytesseract
import cv2
import numpy as np
from PIL import Image
import re
from sentence_transformers import SentenceTransformer
import faiss
from google.colab import files
import tempfile
from googletrans import Translator

# Cargar modelo de embeddings
model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')
translator = Translator()

def extract_text_from_pdf(pdf_path):
    doc = pymupdf.open(pdf_path)
    text = ""
    images = []

    for page in doc:
        page_text = page.get_text("text")
        page_text = page_text.replace("/2018", "")
        page_text = page_text.replace("Seguro 2030: el impacto de la IA en el futuro del seguro |McKinsey & Company", "")
        page_text = page_text.replace("https://www.mckinsey.com/industries/financial-services/our-insights/insurance-2030-the-impact-of-ai-on-the-future-of-insurance", "")
        page_text = re.sub(r'\b\d+/\d+\b', "", page_text)  # Eliminar cadenas como 1/13 hasta 13/13
        text += page_text + "\n"

        for img in page.get_images(full=True):
            xref = img[0]
            base_image = doc.extract_image(xref)
            image = Image.open(base_image["image"])
            images.append(image)
            text += pytesseract.image_to_string(image) + "\n"

    return text, images

def analyze_pie_chart(image):
    image_np = np.array(image)
    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
    circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1.2, 100, param1=50, param2=30, minRadius=50, maxRadius=200)
    if circles is not None:
        return "El documento contiene un gráfico de tipo circular (pie chart)."
    return ""

def create_vector_store(text):
    sentences = list(set([s.strip() for s in text.split(".\n") if len(s.strip()) > 10]))  # Eliminar duplicados
    embeddings = model.encode(sentences, convert_to_numpy=True, normalize_embeddings=True)
    index = faiss.IndexFlatIP(embeddings.shape[1])  # Distancia por producto interno para más precisión
    index.add(embeddings)
    return index, sentences

def search_document(query, index, sentences):
    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    D, I = index.search(query_embedding, k=3)  # Buscar los 3 más relevantes
    results = list(set([sentences[i] for i in I[0] if i < len(sentences)]))  # Evitar repeticiones
    return "\n".join(results) if results else "No encontré información relevante en el documento."

def translate_to_spanish(text):
    return translator.translate(text, dest='es').text

# Subir archivo en Colab
uploaded = files.upload()
for filename, file_data in uploaded.items():
    with open(filename, "wb") as f:
        f.write(file_data)
    pdf_path = filename

    text, images = extract_text_from_pdf(pdf_path)
    index, sentences = create_vector_store(text)

    # Analizar imágenes si existen
    additional_info = ""
    for img in images:
        additional_info += analyze_pie_chart(img) + "\n"

    print("Chat activado. Escribe 'salir' para terminar.")
    while True:
        query = input("Haz una pregunta sobre el documento: ")
        if query.lower() == "salir":
            print("Saliendo del chat.")
            break
        response = search_document(query, index, sentences)
        translated_response = translate_to_spanish(response)
        print(translated_response)
        print("**********************************************")
    if additional_info.strip():
        print("Información adicional detectada en el documento:")
        print(translate_to_spanish(additional_info))